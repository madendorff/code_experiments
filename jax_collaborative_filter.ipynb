{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Simple Collaborative Filtering in Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: The aim of this exploration was learning jax by using it to solve a practical problem - not to find the best solution to the problem.\n",
    "\n",
    "The inspiration for this comes from [Andrew Ng's course on collaborative filtering](https://www.youtube.com/watch?v=9AP-DgFBNP4) - where he trains a model to infer feature preferences from ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, vmap, grad, jit\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A music service has a library of songs and a small number of rated songs to help calibrate a recommendation engine. This notebook shows how to use vanilla jax to simulate training data, train a model and then produce music recommendations based on a limited number of ratings that each new user supplies.\n",
    "\n",
    "Each song as a set of features that describe it. They could include things like genre, label, mood, tempo etc. These features may be explicitly described in the song data (e.g. label), or may have to be learned from people's song ratings or other labels (e.g. mood).\n",
    "\n",
    "The first part of this exploration will focus on the type of features that can be inferred from ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simple simulation to create music ratings for training\n",
    "\n",
    "First off, if we are going to learn features from song ratings we will need some song ratings. We will produce some of these by means of a simulation. In this simulation we create \"clean\" ratings data by building songs and ratings by trusted staff members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "songs = 100\n",
    "features = 3   # e.g. is_jazz, is_new, is_acoustic\n",
    "staff = 5\n",
    "numrounds = 600\n",
    "learning_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax random numbers work differently to numpy random numbers. Random numbers generation is repeatable based on a key.\n",
    "\n",
    "Note: This is a useful feature of jax, but it takes a bit of getting used to. You have to be ultra disciplined in the way that you assign keys as each time you need a key, you have to create a new one. If you accidentally reuse a key you will introduce unwanted correlations into your codes \"random\" behavior. I found the best way to stay organised is to generate a key upfront and then \"split\" it each time I need a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([  0, 121], dtype=uint32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.PRNGKey(121)\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The set of features produced here are simply used to generate sensible ratings to learn from. I used binary features, but this approach would work with continuous variables too. It would require tweaks to work with categorical data. The rows (axis 0) in this array are songs. The columns (axis 1) are features.\n",
    "\n",
    "Note: When I started building models I loved the convenience of pandas. Row and column indexes make pandas data much more descriptive and the high level data manipulation features of pandas are pretty usefull. Nowadays I try to minimized the amount of dependencies in my code. This makes my code more portable and reusable so I started favoring vanilla numpy over pandas. This exploration that I am doing is all about working out if I am going to switch from vanilla numpy to jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[False,  True, False],\n",
       "             [False, False,  True],\n",
       "             [False, False,  True],\n",
       "             [False,  True,  True],\n",
       "             [ True, False,  True],\n",
       "             [ True, False,  True],\n",
       "             [ True, False, False],\n",
       "             [False, False, False],\n",
       "             [ True,  True, False],\n",
       "             [ True, False,  True]], dtype=bool)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, sfkey = random.split(key)\n",
    "song_features = random.bernoulli(sfkey, p=0.3, shape = (songs, features))\n",
    "song_features[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of making clean simulated data we will also describe how each of these features contribute to staff member ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.50482845, -1.0136279 , -0.98340374],\n",
       "             [ 1.3495023 ,  0.6884206 ,  3.3923235 ],\n",
       "             [ 1.0201885 ,  1.091044  ,  2.011397  ],\n",
       "             [-0.5759724 , -0.47560298,  2.3675237 ],\n",
       "             [-0.30214775,  0.78631365,  0.07394962]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, ppkey = random.split(key)\n",
    "people_preferences = random.normal(key=ppkey, shape= (staff, features))\n",
    "people_preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Generate song ratings to use for training.\n",
    "\n",
    "The formula to calculate music ratings is the simple one below: A sum of each feature multiplied by a parameter that describes the user's preference for that feature.\n",
    "\n",
    "The calc_people_ratings function takes a set of features describing a single song and a set of parameters describing multiple users' music preferences to produce a new array dimensioned by song and user.\n",
    "\n",
    "The output of this array is a row for each song and a column for each staff member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.0136279 ,  0.6884206 ,  1.091044  , -0.47560298,\n",
       "               0.78631365],\n",
       "             [-0.98340374,  3.3923235 ,  2.011397  ,  2.3675237 ,\n",
       "               0.07394962],\n",
       "             [-0.98340374,  3.3923235 ,  2.011397  ,  2.3675237 ,\n",
       "               0.07394962],\n",
       "             [-1.9970317 ,  4.0807443 ,  3.1024408 ,  1.8919207 ,\n",
       "               0.8602633 ],\n",
       "             [-0.4785753 ,  4.741826  ,  3.0315852 ,  1.7915514 ,\n",
       "              -0.22819813],\n",
       "             [-0.4785753 ,  4.741826  ,  3.0315852 ,  1.7915514 ,\n",
       "              -0.22819813],\n",
       "             [ 0.50482845,  1.3495023 ,  1.0201885 , -0.5759724 ,\n",
       "              -0.30214775],\n",
       "             [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "               0.        ],\n",
       "             [-0.50879943,  2.0379229 ,  2.1112323 , -1.0515754 ,\n",
       "               0.4841659 ],\n",
       "             [-0.4785753 ,  4.741826  ,  3.0315852 ,  1.7915514 ,\n",
       "              -0.22819813]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def calc_people_ratings(song,people_preferences):\n",
    "    return jnp.sum(song * people_preferences, axis = 1)\n",
    "\n",
    "song_ratings = vmap(calc_people_ratings, in_axes=(0,None))(song_features,people_preferences)\n",
    "song_ratings[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: Jax uses the numpy API, so lots of this code should look familiar to numpy users, the \"vmap\" function is something special to jax.\n",
    "\n",
    "Vmap is actually one of the first things that drew me to jax. As a rather gross generalization \"python is slow\", but numpy is blazingly fast. This means that if you can push performance critical pieces of your code out of python code down into numpy, you can produce highly performant code using python. I use this strategy a lot. Sometimes you have to perform unnatural acts in numpy to push logic down into it and sometimes numpy just isn't expressive enough to do whatever you are trying to do.\n",
    "\n",
    "The above example is a good one. Cartesian products are easy to do using loops or in sql, but they don't come naturally to numpy. I did this cartesian product that combines user data with songs inside a nifty one line function. This nifty one line function is vectorized by the jax vmap function. I also jit compiled it using the @jit decorator. I had previously done tricks like this with numba. Jax offers a lot more utility than numba. For example, I am not sure that I could change the shape of the output array in a numba vectorized function. You will see more of this increased utility later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have some simulated song ratings, we will forget everything we knew about each user's preferences and try to learn them from the ratings.\n",
    "\n",
    "Initialize a new set of random parameters for each user.\n",
    "\n",
    "The goal of training is to optimize these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.1438687 ,  2.1717694 , -0.65337163],\n",
       "             [ 0.8703508 , -1.2910433 , -0.32265836],\n",
       "             [ 0.25262713, -0.81596595,  0.7166114 ],\n",
       "             [ 0.8140667 ,  0.50421727, -1.1207098 ],\n",
       "             [ 0.50041986,  2.4771438 , -0.3017055 ]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, ppkey =random.split(key)\n",
    "people_params = random.normal(key = ppkey, shape = people_preferences.shape)\n",
    "people_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define a loss function. This loss function computes predicted ratings using the learned parameters and compares them with the simulated ratings.\n",
    "This example loss function predicts ratings using the calc function and then compares them to \"known\" simulated ratings and computes mean absolute error.\n",
    "\n",
    "Iteratively optimize the parameters - moving them in the negative direction of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss: 1.4007049\n",
      "iteration 50 loss: 0.881727\n",
      "iteration 100 loss: 0.58476955\n",
      "iteration 150 loss: 0.34664094\n",
      "iteration 200 loss: 0.17135909\n",
      "iteration 250 loss: 0.035004303\n",
      "iteration 300 loss: 0.006438956\n",
      "iteration 350 loss: 0.006438956\n",
      "iteration 400 loss: 0.006438956\n",
      "iteration 450 loss: 0.006438956\n",
      "iteration 500 loss: 0.006438956\n",
      "iteration 550 loss: 0.006438956\n",
      "final loss: 0.006438956\n"
     ]
    }
   ],
   "source": [
    "def loss(params, target, song_data):\n",
    "    pred = vmap(calc_people_ratings, in_axes=(0,None))(song_data, params)\n",
    "    return jnp.mean(jnp.abs(target-pred))\n",
    "\n",
    "for i in range(numrounds):\n",
    "\n",
    "    current_loss = loss(people_params, target=song_ratings, song_data=song_features)\n",
    "    gradient_loss = grad(loss)(people_params, song_ratings, song_features)\n",
    "    people_params = people_params - gradient_loss * learning_rate\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print('iteration',i,'loss:', current_loss)\n",
    "\n",
    "print('final loss:', loss(people_params, target=song_ratings, song_data=song_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above demonstrates how jax differs from other libraries in that it uses functional rather than object oriented programing. It is also less \"batteries included\" than frameworks like sklearn so there is no \"fit\" method.\n",
    "\n",
    "This functional programming style is one of the subjects of my jax test. Just like I am ensuing the convenience of pandas, I am also ensuing the convenience of classes. This is not like giving up chocolate at \"Lent\". Object orientation and encapsulation taught me how to code better. Why revert back to functions? I am testing the hypothesis that classes undermine reuse and complicate testing because they mix state with application logic. I want to reduce dependencies and keep my state away from my application logic. My model state in this case is the people_params. In OO libraries and frameworks these parameters would belong to a model object. I have broader thoughts about making applications faster to build and maintain and these broader thoughts imply a need to take over and manage application state in a very deliberate way. By allowing classes to own state, it undermines my broader goal. I am testing whether jax takes me closer to my broader goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As a quick test just to confirm that the convergence demonstrated above wasn't an illusion created inadvertently by a bug, I will compare the learned parameters with the ones that we used to simulate ratings. The learned parameters look ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DeviceArray([[ 0.00975922,  0.00260186,  0.00837284],\n             [ 0.01275504,  0.00506359,  0.00137758],\n             [ 0.00435984,  0.00620937,  0.01079082],\n             [ 0.00116295,  0.00417939,  0.00263429],\n             [-0.00256795, -0.00603473, -0.0123453 ]], dtype=float32)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_preferences - people_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Understanding new user's music preferences\n",
    "\n",
    "New users supply 3 song ratings. These 3 ratings will be used to learn their music preferences. Unlike in the simulation, this time round I am not going to cheat by learning from cleanly produced ratings. I will infer user preferences from random ratings. I didn't attempt to normalize the ratings. Let's see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(song_ratings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Generate ratings for a subset of songs for new users. I could mimic the skew distribution above when making up ratings for new users, but for a quick test I will go for normally distributed ratings with a mean of zero and see what happens.\n",
    "\n",
    "Each of 2 new users rates the first 3 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2),\n",
       " DeviceArray([[ 0.7965112 , -1.0726503 ],\n",
       "              [ 1.1576934 , -0.60082686],\n",
       "              [ 0.36642167, -0.3806967 ]], dtype=float32))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_members = 2\n",
    "new_ratings = 3\n",
    "\n",
    "key, nmkey = random.split(key)\n",
    "new_member_ratings = random.normal(nmkey,shape= (new_ratings,new_members))\n",
    "new_member_ratings.shape, new_member_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize the parameters for these new users randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.27062193, -0.07210656, -1.2107201 ],\n",
       "             [ 0.01730612, -1.4695468 , -0.29803303]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, npkey = random.split(key)\n",
    "new_member_params = random.normal(key = npkey, shape = (new_members, features))\n",
    "new_member_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Iteratively find the best value of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.87719893\n",
      "50 0.16940932\n",
      "100 0.17883575\n",
      "150 0.16940932\n",
      "200 0.17883575\n",
      "250 0.16940932\n",
      "300 0.17883575\n",
      "350 0.16940932\n",
      "400 0.17883575\n",
      "450 0.16940932\n",
      "500 0.17883575\n",
      "550 0.16940932\n",
      "final loss: 0.16940932\n"
     ]
    }
   ],
   "source": [
    "for i in range(numrounds):\n",
    "\n",
    "    current_loss = loss(new_member_params, target=new_member_ratings, song_data = song_features[0:new_ratings, :])\n",
    "    gradient_loss = grad(loss)(new_member_params, new_member_ratings, song_data = song_features[0:new_ratings, :])\n",
    "    new_member_params = new_member_params - gradient_loss*learning_rate\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        current_loss = loss(new_member_params, target=new_member_ratings, song_data = song_features[0:new_ratings, :])\n",
    "        gradient_loss = grad(loss)(new_member_params, new_member_ratings, song_data = song_features[0:new_ratings, :])\n",
    "        new_member_params = new_member_params - gradient_loss * learning_rate\n",
    "\n",
    "        print(i,current_loss)\n",
    "\n",
    "print('final loss:', loss(new_member_params, target=new_member_ratings, song_data=song_features[0:new_ratings, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice how the loss function didn't converge to zero nearly as well as it did before on the clean rating data. I attribute this to the fact that I used random ratings that don't follow the human mechanics of how real people would rate, and don't even conform to the distribution of the simulated ratings. It is also probably indicative of the natural bias that exists in this model as there there are only 3 binary features, so I will accept it for now.\n",
    "\n",
    "Note: Are you bothered by the fact that I copied and pasted the training logic? I am. In a \"batteries included\" framework where you simply call a fit() method, there is no need to do this. Of course I could have written a fit function, but given the nature of this quick test I allowed myself to commit this infraction. There are libraries that sit on top of jax that at least supply battery holders. I will explore those in the future.\n",
    "\n",
    "## Make Recommendations for new users\n",
    "\n",
    "Use the optimized parameters to rate all of the songs based on the learned preferences for new members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.7945603 , -1.0695472 ],\n",
       "             [ 0.38928005, -0.43136635],\n",
       "             [ 0.38928005, -0.43136635],\n",
       "             [ 1.1838404 , -1.5009135 ],\n",
       "             [ 0.659902  , -0.41406024],\n",
       "             [ 0.659902  , -0.41406024],\n",
       "             [ 0.27062193,  0.01730612],\n",
       "             [ 0.        ,  0.        ],\n",
       "             [ 1.0651822 , -1.0522411 ],\n",
       "             [ 0.659902  , -0.41406024]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_member_ratings = vmap(calc_people_ratings, in_axes=(0,None))(song_features,new_member_params)\n",
    "new_member_ratings[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The array above contains the predicted ratings for all 100 songs for the two new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This simple example showed how to use vanilla jax to train a music recommendation model and produce music recommendations for new users. Of course it doesn't represent the state of the art in recommender systems, but it was still a useful experiment in the use of jax. This is what I learned:\n",
    "\n",
    "1. Once you get your head around what you can do with vmap, it opens up new possibilities for writing highly performant code in python.\n",
    "2. Getting your head around vmap is actually not that easy.\n",
    "3. The auto gradients in jax seem to work well.\n",
    "4. Lack of \"batteries include\" will mean having to write more code/and or understand the full jax ecosystem.\n",
    "5. It was a useful experiment. I would like to extend it. I want to include a naive bayes model for inferring features from other contextual data. I would also like to see whether it could be improved - perhaps made more dynamic - using RLAX for reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}